{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/shiri/OneDrive/Desktop/diagnostics\\dictionary\n"
     ]
    }
   ],
   "source": [
    "main_folder='C:/Users/shiri/OneDrive/Desktop/diagnostics'\n",
    "dictionary_folder='dictionary'\n",
    "\n",
    "dictionary_path=os.path.join(main_folder,dictionary_folder)\n",
    "print(dictionary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(filename='pop.csv',main_folder='C:/Users/shiri/OneDrive/Desktop/diagnostics'):\n",
    "    '''it will check for duplicate rows and single out all \n",
    "    the indices in the csv file which are duplicated'''\n",
    "    \n",
    "    #read in the dataframe\n",
    "    df=pd.read_csv(os.path.join(main_folder,filename))\n",
    "    df.columns.str.lower()\n",
    "    \n",
    "    #get the dataframe where we have duplicates, keep=False will keep both duplicated rows\n",
    "    df_dup=df[df.duplicated(keep=False)]\n",
    "    #groupby all the columns to get groups of duplicated rows\n",
    "    grouped_df=df_dup.groupby(list(df.columns))\n",
    "\n",
    "    #getting tuples of indices for duplicated rows. adding 2 to the index to match the index in csv file\n",
    "    return [tuple(v.index+2) for k,v in grouped_df]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_constraints(filename='pop.csv',dictionary_folder='dictionary',\n",
    "                      main_folder='C:/Users/shiri/OneDrive/Desktop/diagnostics'):\n",
    "    \n",
    "    '''it will go through all the categorical columns and check for constraints violation\n",
    "    by comparing each column to a dictionary file in a specified folder\n",
    "    if there is a violation it will print out the indices of the csv datafile, the column involved \n",
    "    and the values which are not found in the dictionary for that specific column'''\n",
    "    \n",
    "    #separating date columns from the rest of the columns, named here as categ_cols\n",
    "    df=pd.read_csv(os.path.join(main_folder,filename))\n",
    "    df.columns=df.columns.str.lower()\n",
    "\n",
    "    date_cols=[]\n",
    "    categ_cols=[]\n",
    "\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            date_col=str(int(c))\n",
    "            date_cols.append(date_col)\n",
    "        except ValueError:\n",
    "            categ_cols.append(c)\n",
    "            \n",
    "    for f in categ_cols:\n",
    "        try:\n",
    "            df_dictionary=pd.read_excel(os.path.join(main_folder,dictionary_folder,f+'_dictionary.xlsx'))\n",
    "            print(f'checking constraints for column: {f}')\n",
    "            \n",
    "            #check the constraints\n",
    "            allowed_vals=df_dictionary.iloc[:,0].unique()\n",
    "            #get unique values for column f for the main datafile\n",
    "            unique_vals=df[f].unique()\n",
    "            #get the difference in sets to exclude values in the main dataset not found in the dictionary\n",
    "            diff=list(set(df[f].unique()).difference(set(df_dictionary.iloc[:,0].unique())))\n",
    "            #get the index of the main csv file where there is an outlier value\n",
    "            if len(diff)>0:\n",
    "                idx=list(df[df[f].isin(diff)].index+2)\n",
    "                print(f'WARNING !!! outliers in column: {f} and indices {idx} in the csv file')\n",
    "                print(f'WARNING !!! the outliers are: {diff}\\n')\n",
    "            else:\n",
    "                print('no violation for constraints\\n')\n",
    "                \n",
    "        except:\n",
    "            print(f'could not  find {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(filename='pop.csv',main_folder='C:/Users/shiri/OneDrive/Desktop/diagnostics'):\n",
    "    \n",
    "    '''this function will \n",
    "    1-extract the dataframe with dates as columns\n",
    "    2-loop over rows and separate rows with less than 5 datapoints from the rest\n",
    "    3-apply iqr on the rows with >=5 datapoints and incase of anomaly detection, save the row index alongside the column\n",
    "    with the anomaly'''\n",
    "    \n",
    "    #separating date columns from the rest of the columns, named here as categ_cols\n",
    "    df=pd.read_csv(os.path.join(main_folder,filename))\n",
    "    df.columns=df.columns.str.lower()\n",
    "\n",
    "    date_cols=[]\n",
    "    categ_cols=[]\n",
    "\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            #have to convert back to strings because in the original dataframe they are in strings\n",
    "            date_col=str(int(c))\n",
    "            date_cols.append(date_col)\n",
    "        except ValueError:\n",
    "            categ_cols.append(c)\n",
    "            \n",
    "    df_values=df[date_cols]\n",
    "    \n",
    "    #loop over rows to separate rows with <5 datapoints from the rest\n",
    "    idx_gte5=[] #indices list for greater or equal to 5 datapoints\n",
    "    idx_lt5=[]  #indices list for less or equal to 5 datapoints\n",
    "    outlier_idx=[]  #outliers index\n",
    "    outlier_col_idx=[]\n",
    "    \n",
    "    for idx,row in df_values.iterrows():\n",
    "        #use item() to extract the integer from numpy.int64\n",
    "        if row.count().item()>=5:\n",
    "            idx_gte5.append(idx)\n",
    "            #apply IQR on the row\n",
    "            Q1=row.quantile(0.25)\n",
    "            Q3=row.quantile(0.75)\n",
    "            IQR=Q3-Q1\n",
    "\n",
    "            lower_bound = Q1 - 5 * IQR\n",
    "            upper_bound = Q3 + 5 * IQR\n",
    "            \n",
    "            #if there was at least one datapoint outside the range print the index of the row and \n",
    "            #the column index of the outlier\n",
    "            bool=(row<lower_bound) | (row>upper_bound)\n",
    "            if any(bool):\n",
    "                outlier_idx.append(idx)\n",
    "                \n",
    "                #get the column indices for the outliers\n",
    "                col_idx=list(row[bool].index)\n",
    "                outlier_col_idx.append(col_idx)\n",
    "            \n",
    "        else:\n",
    "            idx_lt5.append(idx)\n",
    "    \n",
    "    #write the dataframe with outliers to an excel file\n",
    "    df_outlier=df.iloc[outlier_idx]\n",
    "    #add the column indices as a column to the dataframe\n",
    "    df_outlier['column_indices']=outlier_col_idx\n",
    "    df_outlier.to_excel(main_folder+'/outliers.xlsx', index=False, sheet_name='Sheet1')\n",
    "    \n",
    "    #write the dataframe with less than 5 datapoints to an excel file\n",
    "    df_lt5=df.iloc[idx_lt5]\n",
    "    df_lt5.to_excel(main_folder+'/df_lt5.xlsx', index=False, sheet_name='Sheet1')\n",
    "    #print out the indicator names\n",
    "    unique_inds=df_lt5['indicator_name'].unique()\n",
    "    print('the indicators below have less than 5 datapoints:\\n')\n",
    "    print(unique_inds)\n",
    "    print('------------------------------------------------')\n",
    "    \n",
    "    df_gte5=df.iloc[idx_gte5]\n",
    "    df_values_gte5=df_values.iloc[idx_gte5]\n",
    "    \n",
    "    #apply IQR for each row\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiri\\AppData\\Local\\Temp\\ipykernel_11628\\243591640.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_outlier['column_indices']=outlier_col_idx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the indicators below have less than 5 datapoints:\n",
      "\n",
      "['Average annual population growth rate (percent)'\n",
      " 'Average household size (number)' 'Average number of persons per room'\n",
      " 'Households headed by women (percent)'\n",
      " 'Mean age at first marriage (years)' 'Registered marriages (number)'\n",
      " 'Registered divorces (number)' 'Registered deaths (number)'\n",
      " 'Total fertility rate (children per woman)' 'Refugee population (number)'\n",
      " 'International migrant stock (number)'\n",
      " 'Population distribution by marital status (percentage)'\n",
      " 'Population size (number)'\n",
      " 'Population distribution by marital status (number)'\n",
      " 'Registered livebirths (number)' 'Infant mortality rate (per 1'\n",
      " 'Under-five mortality rate (per 1' 'Causes of death (percent)']\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iqr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
