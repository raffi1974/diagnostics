{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(filename='pop_test.csv',main_folder='C:/Users/shiri/OneDrive/Desktop/diagnostics'):\n",
    "    '''it will check for duplicate rows and single out all \n",
    "    the indices in the csv file which are duplicated'''\n",
    "    \n",
    "    #read in the dataframe\n",
    "    df=pd.read_csv(os.path.join(main_folder,filename))\n",
    "    df.columns.str.lower()\n",
    "    \n",
    "    #get the dataframe where we have duplicates, keep=False will keep both duplicated rows\n",
    "    df_dup=df[df.duplicated(keep=False)]\n",
    "    #groupby all the columns to get groups of duplicated rows\n",
    "    grouped_df=df_dup.groupby(list(df.columns))\n",
    "\n",
    "    #getting tuples of indices for duplicated rows. adding 2 to the index to match the index in csv file\n",
    "    return [tuple(v.index+2) for k,v in grouped_df]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_constraints(filename='pop_test.csv',dictionary_folder='dictionary',\n",
    "                      main_folder='C:/Users/shiri/OneDrive/Desktop/diagnostics'):\n",
    "    \n",
    "    '''it will go through all the categorical columns and check for constraints violation\n",
    "    by comparing each column to a dictionary file in a specified folder\n",
    "    if there is a violation it will print out the indices of the csv datafile, the column involved \n",
    "    and the values which are not found in the dictionary for that specific column'''\n",
    "    \n",
    "    #separating date columns from the rest of the columns, named here as categ_cols\n",
    "    df=pd.read_csv(os.path.join(main_folder,filename))\n",
    "    df.columns=df.columns.str.lower()\n",
    "\n",
    "    date_cols=[]\n",
    "    categ_cols=[]\n",
    "\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            date_col=str(int(c))\n",
    "            date_cols.append(date_col)\n",
    "        except ValueError:\n",
    "            categ_cols.append(c)\n",
    "            \n",
    "    for f in categ_cols:\n",
    "        try:\n",
    "            df_dictionary=pd.read_excel(os.path.join(main_folder,dictionary_folder,f+'_dictionary.xlsx'))\n",
    "            print(f'checking constraints for column: {f}')\n",
    "            \n",
    "            #check the constraints\n",
    "            allowed_vals=df_dictionary.iloc[:,0].unique()\n",
    "            #get unique values for column f for the main datafile\n",
    "            unique_vals=df[f].unique()\n",
    "            #get the difference in sets to exclude values in the main dataset not found in the dictionary\n",
    "            diff=list(set(df[f].unique()).difference(set(df_dictionary.iloc[:,0].unique())))\n",
    "            #get the index of the main csv file where there is an outlier value\n",
    "            if len(diff)>0:\n",
    "                idx=list(df[df[f].isin(diff)].index+2)\n",
    "                print(f'WARNING !!! outliers in column: {f} and indices {idx} in the csv file')\n",
    "                print(f'WARNING !!! the outliers are: {diff}\\n')\n",
    "            else:\n",
    "                print('no violation for constraints\\n')\n",
    "                \n",
    "        except:\n",
    "            print(f'could not  find {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_constraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(filename='pop_test.csv',main_folder='C:/Users/shiri/OneDrive/Desktop/diagnostics',\n",
    "        mad_multiplier=3, iqr_multiplier=3):\n",
    "    \n",
    "    '''this function will \n",
    "    1-extract the dataframe with dates as columns\n",
    "    2-loop over rows and separate rows with less than 5 datapoints from the rest\n",
    "    3-apply iqr and MAD on the rows with >=5 datapoints and incase of both detect anomaly, \n",
    "    save the row index alongside the column with the anomaly'''\n",
    "    \n",
    "    #separating date columns from the rest of the columns, named here as categ_cols\n",
    "    df=pd.read_csv(os.path.join(main_folder,filename))\n",
    "    df.columns=df.columns.str.lower()\n",
    "\n",
    "    date_cols=[]\n",
    "    categ_cols=[]\n",
    "\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            #have to convert back to strings because in the original dataframe they are in strings\n",
    "            date_col=str(int(c))\n",
    "            date_cols.append(date_col)\n",
    "        except ValueError:\n",
    "            categ_cols.append(c)\n",
    "            \n",
    "    df_values=df[date_cols]\n",
    "    \n",
    "    #loop over rows to separate rows with <5 datapoints from the rest\n",
    "    idx_gte5=[] #indices list for greater or equal to 5 datapoints\n",
    "    idx_lt5=[]  #indices list for less or equal to 5 datapoints\n",
    "    outlier_idx=[]  #outliers index\n",
    "    outlier_col_idx=[]\n",
    "    \n",
    "    for idx,row in df_values.iterrows():\n",
    "        #use item() to extract the integer from numpy.int64\n",
    "        if row.count().item()>=5:\n",
    "            \n",
    "            # Drop NaNs from the row\n",
    "            cleaned_row = row.dropna()\n",
    "            idx_gte5.append(idx)\n",
    "            #apply IQR on the row\n",
    "            Q1=cleaned_row.quantile(0.25)\n",
    "            Q3=cleaned_row.quantile(0.75)\n",
    "            IQR=Q3-Q1\n",
    "\n",
    "            lower_bound = Q1 - iqr_multiplier * IQR\n",
    "            upper_bound = Q3 + iqr_multiplier * IQR\n",
    "            \n",
    "            #if there was at least one datapoint outside the range print the index of the row and \n",
    "            #the column index of the outlier\n",
    "            bool_iqr=(cleaned_row<lower_bound) | (cleaned_row>upper_bound)\n",
    "            \n",
    "            #apply MAD\n",
    "            median = cleaned_row.median()\n",
    "            absolute_deviation = np.abs(cleaned_row - median)\n",
    "            mad = absolute_deviation.mean()\n",
    "            \n",
    "            median = row.median()\n",
    "            threshold = mad * mad_multiplier\n",
    "            absolute_deviation = np.abs(cleaned_row - median)\n",
    "            # Identify outliers\n",
    "            bool_mad = absolute_deviation > threshold\n",
    "            \n",
    "            #if both vote true then it then the row considered to have an outlier\n",
    "            bool=bool_iqr*bool_mad\n",
    "            \n",
    "            if any(bool):\n",
    "                outlier_idx.append(idx)\n",
    "                \n",
    "                #get the column indices for the outliers\n",
    "                col_idx=list(cleaned_row[bool].index)\n",
    "                outlier_col_idx.append(col_idx)\n",
    "            \n",
    "        else:\n",
    "            idx_lt5.append(idx)\n",
    "    \n",
    "    #write the dataframe with outliers to an excel file\n",
    "    df_outlier=df.iloc[outlier_idx].copy()\n",
    "    #add the column indices as a column to the dataframe\n",
    "    df_outlier['column_indices']=outlier_col_idx\n",
    "    df_outlier.to_excel(main_folder+'/outliers.xlsx', index=False, sheet_name='Sheet1')\n",
    "    \n",
    "    #write the dataframe with less than 5 datapoints to an excel file\n",
    "    df_lt5=df.iloc[idx_lt5].copy()\n",
    "    df_lt5.to_excel(main_folder+'/df_lt5.xlsx', index=False, sheet_name='Sheet1')\n",
    "    #print out the indicator names\n",
    "    unique_inds=df_lt5['indicator_name'].unique()\n",
    "    print('the indicators below have less than 5 datapoints:\\n')\n",
    "    print(unique_inds)\n",
    "    print('------------------------------------------------')\n",
    "    \n",
    "    df_gte5=df.iloc[idx_gte5]\n",
    "    df_values_gte5=df_values.iloc[idx_gte5]\n",
    "    \n",
    "    #apply IQR for each row\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the indicators below have less than 5 datapoints:\n",
      "\n",
      "['Registered deaths (number)' 'Population size (number)'\n",
      " 'Causes of death (percent)']\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "detect_outliers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "5    False\n",
       "6    False\n",
       "7    False\n",
       "8    False\n",
       "9     True\n",
       "Name: A, dtype: bool"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame with NaN values\n",
    "data = {\n",
    "    'A': [10, np.nan, 3, 4,8,7,4,5,6,60],\n",
    "    'B': [10, np.nan, 3, 4,8,7,4,5,6,60],\n",
    "    'C': [10, np.nan, 3, 4,8,7,4,5,6,60]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data).T\n",
    "\n",
    "df1=df.iloc[0,:]\n",
    "\n",
    "cleaned_row = df1.dropna()\n",
    "if len(cleaned_row) == 0:\n",
    "    np.nan\n",
    "median = cleaned_row.median()\n",
    "absolute_deviation = np.abs(cleaned_row - median)\n",
    "mad = absolute_deviation.mean()\n",
    "print(mad)\n",
    "\n",
    "median = df1.median()\n",
    "threshold = mad * 3\n",
    "# Drop NaNs and find absolute deviations\n",
    "cleaned_row = df1.dropna()\n",
    "absolute_deviation = np.abs(cleaned_row - median)\n",
    "# Identify outliers\n",
    "bool_mad = absolute_deviation > threshold\n",
    "bool_mad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "5    False\n",
       "6    False\n",
       "7    False\n",
       "8    False\n",
       "9     True\n",
       "Name: A, dtype: bool"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_row = df1.dropna()\n",
    "\n",
    "#apply IQR on the row\n",
    "Q1=cleaned_row.quantile(0.25)\n",
    "Q3=cleaned_row.quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "\n",
    "lower_bound = Q1 - 3 * IQR\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "#if there was at least one datapoint outside the range print the index of the row and \n",
    "#the column index of the outlier\n",
    "bool_iqr=(cleaned_row<lower_bound) | (cleaned_row>upper_bound)\n",
    "bool_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
